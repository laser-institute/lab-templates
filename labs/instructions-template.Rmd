---
title: "LASER Lab Template"
author: "LASER Team"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Style guide:

-   Level 1 header not used. Only for Lab title
-   Level 2 Headers are main headers and in all caps and identify step in LA workflow addressed
-   Level 3 Headers for main topic covered under each workflow process and follow tidyverse Verb Noun format, e.g. Import Data, Tidy Text,
-   Level 4 & 5 headers for subtopics format as you please. And don't be afraid to overuse.
-   Bulleted and numbered lists use tight layout
-   when introducing new packages provide a solid description, use hex sticker of available and set width to 20%
-   use your own discretion for other images
-   when introducing new functions provide a very thorough explanation of the function and key arguments used
-   Label code chunks with one word or hyphenated two word description like "import-csv"\

## 1. PREPARE

Start each lab by providing a brief but engaging overview of the topic (e.g., text mining) that you'll be covering over the next two days.

Follow the overview with a quick bulleted list introducing each lab and what they will cover. For example:

-   **Lab 1: Tidy Text, Tokens, & Twitter**. We take a closer look at the literature and research questions that will be guiding our analysis; importing data through Twitter's developer API; and wrangling our data into a one-token-per-row tidy text format.
-   **Lab 2: Word Counts, Clouds & Frequencies.** For our second lab, we use simple summary statistics and data visualization to explore our data and see what insight it provides in response to our questions.
-   **Lab 3: Sentiment Analysis & School Reform**. We focus on the use of lexicons to compare the sentiment of tweets about the [NGSS](https://www.nextgenscience.org/) and [CCSS](http://www.corestandards.org/) state standards in order to better understand public reaction to these two curriculum reform efforts. 
-   **Lab 4: Topic Models & Tea Leaves.** In our final lab we introduce an approach to identify "topics" by examining how words cohere into different latent themes based on patterns of co-occurrence of words within documents.

For subsequent labs, provide just a brief overview of the primary topics covered by the lab, along with a with a bulleted list introducing primary topics. Lab 4 for text mining for example, would be something like:

1.  **Prepare**: In this section we look at an extension of the tidy text workflow and are introduced to the `topicmodels` and `stm` packages for topic modeling.
2.  **Wrangle**: We revisit tidying and tokenizing text but are also introduced to the the `stm` package. This package makes use of `tm`text mining package to preprocess text and will also be our first introduction to word stemming document term matrices.
3.  **Model**: We take a look at two different approaches to topic modeling: Latent Dirichlet Allocation (LDA) and Structural Topic Modeling (STM), which is very similar to LDA but can use metadata about documents to improve the assignment of words to "topics" and examine relationships between topics and covariates. 
4.  **Explore**: To further explore the results of our topic model, we use several handy functions from including the `findThoughts` function for viewing documents assigned to a given topic and the `toLDAvis` function for exploring topic and word distributions.

### 1a. Understand the Context

In Lab 1 you may want to introduce LASER scholars to the study and/or data that will be guiding your first lab and subsequent labs. See example here:

<https://sbkellogg.github.io/eci-588/unit-2/unit-2-walkthrough.html#1a_Some_Context>

In subsequent labs this may be simply a reminder of the literature or data or what they are trying to accomplish with the data and and any new background information that might be needed and is not shared in the opening paragraph(s).

It can also be omitted or just folded into introduction at beginning and save hit

#### Article Title

#### Abstract

Copy and paste abstract...

#### Data and Analysis

Overview of data and analysis and how this guides the analysis they will be performing and the data they will be using...

#### Key Findings

List here...\

### 1b. Define Research Questions

This section instructed

\

Text Mining Overview

### 1c. Load Packages

#### Prior Packages

Brief intro text...

Instruct participants to type or copy and paste the code into their console or add to the exercise file since they will need these packages for "Your Turn" exercises:

```{r load-packages}
library(dplyr)
library(readr)
library(tidyr)
```

#### New Packages

Introduce any new packages you'll be using in this lab that they will need immediately and provide a decent explanation of the package. If you're introducing several new packages, they can be introduced in later sections where appropriate.

#### tidytext

![](img/tidytext.png){width="20%"}

Text data, by it's very nature is ESPECIALLY untidy and sometimes referred to as "unstructured." The `tidytext`package provide functions and supporting data sets to allow conversion of text to and from tidy formats, and to switch seamlessly between tidy tools and existing text mining packages.

As we'll learn later in this lab, using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use. Much of the infrastructure needed for text mining with tidy data frames already exists in `tidyverse` packages with which we've already been introduced.

For a more comprehensive introduction to the `tidytext` package, we cannot recommend enough the free online book, [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com) by Silge and Robinson (2018).

Let's go ahead and load `tidytext`:

```{r}
library(tidytext)
```

## 2. WRANGLE

Provide a 1-paragraph or longer overview to your wrangle section along with bulleted intro of key topics that will also be your level 3 headers like so:

a.  **Import Data**. In this section, we introduce the `rtweet` package and some key functions to search for tweets or users of interest.
b.  **Tidy Tweets**. We revisit the `tidytext` package to both "tidy" and tokenize our tweets in order to create our data frame for analysis.
c.  **Get Sentiments**. We conclude our data wrangling by introducing sentiment lexicons and the `inner_join()` function for appending sentiment values to our data frame.

### 2a. Import Data

Intro text....

Instruction and explanation of code chunk if needed ...

```{r import-tweets, eval=FALSE}
ccss_tweets <- read_csv("data/ccss-tweets.csv")
```

Further explanation of code chunk if needed, especially if a new function...

Further explanation of data if needed, e.g. codebook or explanatory text and/or link to description of data source...

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### [Your Turn]{style="color:green"} ⤵ 

Intro text and instructions...

If needed, list tasks and provide hints or examples of expected outputs:

##### Example:

For the remainder of the lab, you'll be asked to use your own Twitter data. Complete the following steps before proceeding to the 2b. Tidy Text section:

1.  Create a new code chunk and write a query based on a STEM area of interest.
2.  Subset your data to remove any unnecessary tweets from analysis.
3.  Assign your search to a new object called `my_tweets`.
4.  Output your new dataset using the `datatable()` function from the `DT` package and take a quick look.

Extra credit for using the `%>%` pipe operator and efficient use of arguments to keep your code succinct and using the `<-` assignment operator only once.

You're output should look something like this:

### 2b. Tidy Data

Intro text....

Instruction and explanation of code chunk if needed ...

```{r}

```

Further explanation of code chunk if needed...

##### Example:

The `tidytext` package provides the incredibly powerful `unnest_tokens()` function to tokenize text (including tweets!) and convert them to a one-token-per-row format.

Let's tokenize our tweets by using this function to split each tweet into a single row to make it easier to analyze:

```{r unnest-tokens, eval=FALSE}
ccss_unigrams <- unnest_tokens(ccss_tweets, 
                               output = word, 
                               input = text,
                               token = "tweets")
```

There is A LOT to unpack with this function. First notice that `unnest_tokens()` expects a data frame as the first argument, followed by two column names. The second argument is an output column name that doesn't currently exist but will be created as the text is "unnested" into it (`word`, in this case). This is followed by the input column that the text comes from, which we uncreatively named `text`. Also notice:

-   By default, a token is an individual word or unigram.

-   Other columns, such as `id` and `created_at`, are retained.

-   All punctuation has been removed.

-   Tokens have been changed to lowercase, which makes them easier to compare or combine with other datasets (use the `to_lower = FALSE` argument to turn off if desired).

The `unnest_tokens()` function also has specialized `“tweets”` tokenizer in the `tokens =` argument that is very useful for dealing with Twitter text in that it retains hashtags and mentions of usernames with the \@ symbol.

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

### 2c. Transform Data

Intro text....

Instruction and explanation of code chunk if needed ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

## 3. EXPLORE

Provide a 1-paragraph or longer overview to your explore section along with bulleted intro of key topics covered like so:

a.  **Main Topic 1**. Loren ipsum dolor...
b.  **Main Topic 2**. Loren ipsum dolor...
c.  **Main Topic 3**. Loren ipsum dolor...
d.  **Main Topic 4**. Loren ipsum dolor...

### 3a. Main Topic

Intro text....

Instruction and explanation of code chunk if needed ...

```{r}

```

Further explanation of code chunk if needed, especially if a new function...

Further explanation of data if needed, e.g. codebook or explanatory text and/or link to description of data source...

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

### 3b. Main Topic

Intro text....

Instruction and explanation of code chunk if needed ...

```{r}

```

Further explanation of code chunk if needed, especially if a new function...

Further explanation of data if needed, e.g. codebook or explanatory text and/or link to description of data source...

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

### 3c. Main Topic

Intro text....

Instruction and explanation of code chunk if needed ...

```{r}

```

Further explanation of code chunk if needed, especially if a new function...

Further explanation of data if needed, e.g. codebook or explanatory text and/or link to description of data source...

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

\

## 4. MODEL

Provide a 1-paragraph or longer overview to your explore section along with bulleted intro of key topics covered like so:

a.  **Main Topic 1**. Loren ipsum dolor...
b.  **Main Topic 2**. Loren ipsum dolor...
c.  **Main Topic 3**. Loren ipsum dolor...
d.  **Main Topic 4**. Loren ipsum dolor...

### 4a. Main Topic

Intro text....

Instruction and explanation of code chunk if needed ...

```{r}

```

Further explanation of code chunk if needed, especially if a new function...

Further explanation of data if needed, e.g. codebook or explanatory text and/or link to description of data source...

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

### 3b. Main Topic

Intro text....

Instruction and explanation of code chunk if needed ...

```{r}

```

Further explanation of code chunk if needed, especially if a new function...

Further explanation of data if needed, e.g. codebook or explanatory text and/or link to description of data source...

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

### 3c. Main Topic

Intro text....

Instruction and explanation of code chunk if needed ...

```{r}

```

Further explanation of code chunk if needed, especially if a new function...

Further explanation of data if needed, e.g. codebook or explanatory text and/or link to description of data source...

#### Subtopic 1 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...

#### Subtopic 2 (If needed)

Intro text...

Instruction and explanation of code chunk ...

```{r}

```

Further explanation of code chunk if needed...
